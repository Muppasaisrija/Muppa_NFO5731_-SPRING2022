{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Muppasaisrija/Muppa_NFO5731_-SPRING2022/blob/main/INFO5731_Assignment_Two_1_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Two**\n",
        "\n",
        "In this assignment, you will try to gather text data from open data source via web scraping or API. After that you need to clean the text data and syntactic analysis of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(40 points). Write a python program to collect text data from **either of the following sources** and save the data into a **csv file**:\n",
        "\n",
        "(1) Collect all the customer reviews of the product [Apple iPhone 11](https://www.amazon.com/Apple-iPhone-11-64GB-Unlocked/dp/B07ZPKF8RG/ref=sr_1_13?dchild=1&keywords=iphone+12&qid=1631721363&sr=8-13) on amazon.\n",
        "\n",
        "(2) Collect the top 10000 User Reviews of the film [Shang-Chi and the Legend of the Ten Rings](https://www.imdb.com/title/tt9376612/reviews?ref_=tt_sa_3) from IMDB.\n",
        "\n",
        "(3) Collect all the reviews of the top 100 most popular software from [G2](https://www.g2.com/) or [Capterra](https://www.capterra.com/)\n",
        "\n",
        "(4) Collect the abstracts of the top 10000 research papers by using the query [natural language processing](https://citeseerx.ist.psu.edu/search?q=natural+language+processing&submit.x=0&submit.y=0&sort=rlv&t=doc) from CiteSeerX.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the [Densho Digital Repository](https://ddr.densho.org/narrators/).\n",
        "\n",
        "(6) Collect the top 10000 tweets by using hashtag [\"#blacklivesmatter\"](https://twitter.com/hashtag/blacklivesmatter) from Twitter. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PuFPKhC0m1fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "47d4fc85-7a59-4cf5-ec35-822ca3206e76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.1.2-py3-none-any.whl (963 kB)\n",
            "\u001b[?25l\r\u001b[K     |▍                               | 10 kB 27.9 MB/s eta 0:00:01\r\u001b[K     |▊                               | 20 kB 24.2 MB/s eta 0:00:01\r\u001b[K     |█                               | 30 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 40 kB 18.1 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 51 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |██                              | 61 kB 12.1 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 71 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 81 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |███                             | 92 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 102 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 112 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████                            | 122 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 133 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 143 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████                           | 153 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 163 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 174 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 184 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 194 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 204 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 215 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 225 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 235 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 245 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 256 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 266 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 276 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 286 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 296 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 307 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 317 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 327 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 337 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 348 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 358 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 368 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 378 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 389 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 399 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 409 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 419 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 430 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 440 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 450 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 460 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 471 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 481 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 491 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 501 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 512 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 522 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 532 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 542 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 552 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 563 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 573 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 583 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 593 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 604 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 614 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 624 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 634 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 645 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 655 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 665 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 675 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 686 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 696 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 706 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 716 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 727 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 737 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 747 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 757 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 768 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 778 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 788 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 798 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 808 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 819 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 829 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 839 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 849 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 860 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 870 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 880 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 890 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 901 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 911 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 921 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 931 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 942 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 952 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 962 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 963 kB 12.8 MB/s \n",
            "\u001b[?25hCollecting trio-websocket~=0.9\n",
            "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
            "Collecting trio~=0.17\n",
            "  Downloading trio-0.20.0-py3-none-any.whl (359 kB)\n",
            "\u001b[K     |████████████████████████████████| 359 kB 67.3 MB/s \n",
            "\u001b[?25hCollecting urllib3[secure,socks]~=1.26\n",
            "  Downloading urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 69.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (21.4.0)\n",
            "Collecting async-generator>=1.9\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n",
            "Collecting outcome\n",
            "  Downloading outcome-1.1.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Collecting wsproto>=0.14\n",
            "  Downloading wsproto-1.1.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (2021.10.8)\n",
            "Collecting cryptography>=1.3.4\n",
            "  Downloading cryptography-36.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 59.3 MB/s \n",
            "\u001b[?25hCollecting pyOpenSSL>=0.14\n",
            "  Downloading pyOpenSSL-22.0.0-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 2.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (2.21)\n",
            "Collecting h11<1,>=0.9.0\n",
            "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 3.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from h11<1,>=0.9.0->wsproto>=0.14->trio-websocket~=0.9->selenium) (3.10.0.2)\n",
            "Installing collected packages: sniffio, outcome, h11, cryptography, async-generator, wsproto, urllib3, trio, pyOpenSSL, trio-websocket, selenium\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.8 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed async-generator-1.10 cryptography-36.0.1 h11-0.13.0 outcome-1.1.0 pyOpenSSL-22.0.0 selenium-4.1.2 sniffio-1.2.0 trio-0.20.0 trio-websocket-0.9.2 urllib3-1.26.8 wsproto-1.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n",
            "Get:1 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:2 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:5 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Hit:6 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:7 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:9 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Ign:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [696 B]\n",
            "Hit:13 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,035 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,252 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [840 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,596 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,474 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [806 kB]\n",
            "Get:21 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [76.8 kB]\n",
            "Get:22 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,827 kB]\n",
            "Get:23 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [936 kB]\n",
            "Get:24 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [42.8 kB]\n",
            "Get:26 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [931 kB]\n",
            "Fetched 15.1 MB in 3s (4,839 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-470\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 75 not upgraded.\n",
            "Need to get 95.3 MB of archives.\n",
            "After this operation, 327 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 97.0.4692.71-0ubuntu0.18.04.1 [1,142 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 97.0.4692.71-0ubuntu0.18.04.1 [84.7 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 97.0.4692.71-0ubuntu0.18.04.1 [4,370 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 97.0.4692.71-0ubuntu0.18.04.1 [5,055 kB]\n",
            "Fetched 95.3 MB in 1s (70.2 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 155320 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_97.0.4692.71-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (97.0.4692.71-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_97.0.4692.71-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (97.0.4692.71-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_97.0.4692.71-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (97.0.4692.71-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_97.0.4692.71-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (97.0.4692.71-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (97.0.4692.71-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (97.0.4692.71-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (97.0.4692.71-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (97.0.4692.71-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n"
          ]
        }
      ],
      "source": [
        "import requests, openpyxl\n",
        "!pip install selenium\n",
        "!pip install beautifulsoup4\n",
        "!apt-get update\n",
        "from bs4 import BeautifulSoup as bs\n",
        "!apt install chromium-chromedriver\n",
        "import urllib.request as req\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "import pandas as pd\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait as wait\n",
        "from selenium import webdriver \n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('-headless')\n",
        "options.add_argument('-no-sandbox')\n",
        "options.add_argument('-disable-dev-shm-usage')"
      ],
      "metadata": {
        "id": "onqTPuFJltfY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titles = [] \n",
        "reviews= []\n",
        "link = 'https://www.imdb.com/title/tt9376612/reviews?ref_=tt_sa_3'\n",
        "d = webdriver.Chrome('chromedriver',options=options) #creating a driver path\n",
        "d.get(link)\n",
        "for num in range(43):         \n",
        "  d.find_element_by_class_name(\"ipl-load-more__button\").click() \n",
        "  time.sleep(5)\n",
        "  LTitle = d.find_elements(By.CLASS_NAME, \"title\") #Using Find_elements to get titles of the movie\n",
        "  LReviews = d.find_elements(By.CLASS_NAME, \"text\") #Using Find_elements to get reviews of the movie\n",
        "  \n",
        "for e, r in zip(LTitle, LReviews):            # appending all reviews and titles into the empty arrays\n",
        "      titles.append((e.text).replace('\\n',''))\n",
        "      reviews.append(r.text)\n",
        "      \n",
        "data = pd.DataFrame(list(zip(titles, reviews)), columns =['Title_of_the_Review', 'Review_of_the_movie'])\n",
        "\n",
        "print(\"Length of data frame is\",len(data))\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCYIKnqwl0zc",
        "outputId": "b44e6907-4df7-4987-c09f-060bd27a93ac"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: find_element_by_class_name is deprecated. Please use find_element(by=By.CLASS_NAME, value=name) instead\n",
            "  import sys\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of data frame is 1099\n",
            "                                    Title_of_the_Review                                Review_of_the_movie\n",
            "0                                       A visual feast.                                                   \n",
            "1                                         MCU and funny  Shaun (Simu Liu) and Katy (Awkwafina) are slac...\n",
            "2                                            Next Phase                                                   \n",
            "3                                               Bus Boy                                                   \n",
            "4                                                  Wow!  I was not expecting that, I had visions of a f...\n",
            "...                                                 ...                                                ...\n",
            "1094  Decent superhero flick, but shallow plot and w...  It's a decent fun adventure movie, and it is v...\n",
            "1095  This actually might be the best superhero movi...                                                   \n",
            "1096                                           AMAZING!  HUGE step-up from Black Widow. This movie made...\n",
            "1097                            Very pleasing to watch.  Most beautiful Marvel movie there's ever been....\n",
            "1098                                          Loved it.  It's a nice classic Marvel movie with great im...\n",
            "\n",
            "[1099 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points). Write a python program to **clean the text data** you collected above and save the data in a new column in the csv file. The data cleaning steps include:\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the [stopwords list](https://gist.github.com/sebleier/554280).\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming. \n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vATjQNTY8buA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bff4e6ac-a204-4b00-e389-cbc016a5b12d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# nltk.download('data')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "import numpy as np\n",
        "import nltk.corpus\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8CF9C35nIWo",
        "outputId": "7102091a-9f5b-44b9-82ca-f96db635d1e7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=data\n",
        "df=df[df[\"Review_of_the_movie\"]!= \"\"] #Removing null review\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bn4GorsDnVEX",
        "outputId": "b5887380-f035-4445-d7a9-9f4ba98e3f7b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                    Title_of_the_Review                                Review_of_the_movie\n",
            "1                                         MCU and funny  Shaun (Simu Liu) and Katy (Awkwafina) are slac...\n",
            "4                                                  Wow!  I was not expecting that, I had visions of a f...\n",
            "5                                Precious ... ten times  Of course I am just riffing off, teasing and m...\n",
            "8                       The best Marvel movie so far...  Having seen the trailer for the 2021 Marvel ac...\n",
            "9                    Crouching Tiger, Hidden Superhero.  Martial arts meets the MCU in Shang-Chi and th...\n",
            "...                                                 ...                                                ...\n",
            "1093                          A Unique Twist in the MCU  The cast, the costumes, the setting! To me, in...\n",
            "1094  Decent superhero flick, but shallow plot and w...  It's a decent fun adventure movie, and it is v...\n",
            "1096                                           AMAZING!  HUGE step-up from Black Widow. This movie made...\n",
            "1097                            Very pleasing to watch.  Most beautiful Marvel movie there's ever been....\n",
            "1098                                          Loved it.  It's a nice classic Marvel movie with great im...\n",
            "\n",
            "[930 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stpwordlist = stopwords.words('english')\n",
        "snow_stemmer = SnowballStemmer(\"english\")\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def clean(df):\n",
        "    #Remove noise, such as special characters and punctuations.\n",
        "    df.loc[:,[\"clean_txt\"]]=df[\"Review_of_the_movie\"].apply(lambda ele: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", ele))\n",
        "    df.loc[:,[\"clean_txt\"]]=df[\"clean_txt\"].apply(lambda ele: re.sub(r\"\\d+\", \"\", ele))   #Remove numbers\n",
        "    df.loc[:,[\"clean_txt\"]]=df[\"clean_txt\"].apply(lambda string: ' '.join([w for w in string.split() if w not in (stpwordlist)])) #Remove stopwords by using the stopwords list\n",
        "    df.loc[:,[\"clean_txt\"]]=df[\"clean_txt\"].str.lower()  #Lowercase all texts\n",
        "    df.loc[:,[\"tokens\"]] = df[\"clean_txt\"].apply(lambda x: word_tokenize(x))  #word tokenize\n",
        "    def word_stemmer(text):\n",
        "      stem_text = [snow_stemmer.stem(i) for i in text]\n",
        "      return stem_text\n",
        "    df.loc[:,['text_stem']] = df[\"tokens\"].apply(lambda x: word_stemmer(x))  #Stemming\n",
        "    def word_lemmatizer(text):\n",
        "      lem_text = [lemmatizer.lemmatize(i) for i in text]\n",
        "      return lem_text\n",
        "    df.loc[:,['text_lem']] = df[\"tokens\"].apply(lambda x: word_lemmatizer(x))  # Lemmatization\n",
        "    return df\n",
        "c = clean(df)\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QsdVXXcndzQ",
        "outputId": "5fa990a2-36f7-4ec5-a85a-d2033406de49"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1773: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self._setitem_single_column(ilocs[0], value, pi)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                    Title_of_the_Review  ...                                           text_lem\n",
            "1                                         MCU and funny  ...  [shaun, simu, liu, katy, awkwafina, slacker, s...\n",
            "4                                                  Wow!  ...  [i, expecting, i, vision, film, along, line, s...\n",
            "5                                Precious ... ten times  ...  [of, course, i, riffing, teasing, making, fun,...\n",
            "8                       The best Marvel movie so far...  ...  [having, seen, trailer, marvel, action, movie,...\n",
            "9                    Crouching Tiger, Hidden Superhero.  ...  [martial, art, meet, mcu, shangchi, legend, te...\n",
            "...                                                 ...  ...                                                ...\n",
            "1093                          A Unique Twist in the MCU  ...  [the, cast, costume, setting, to, opinion, des...\n",
            "1094  Decent superhero flick, but shallow plot and w...  ...  [it, decent, fun, adventure, movie, much, run,...\n",
            "1096                                           AMAZING!  ...  [huge, stepup, black, widow, this, movie, made...\n",
            "1097                            Very pleasing to watch.  ...  [most, beautiful, marvel, movie, there, ever, ...\n",
            "1098                                          Loved it.  ...  [it, nice, classic, marvel, movie, great, imag...\n",
            "\n",
            "[930 rows x 6 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(30 points). Write a python program to conduct **syntax and structure analysis** of the clean text you just saved above. The syntax and structure analysis includes: \n",
        "\n",
        "(1) Parts of Speech (POS) Tagging: Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) Named Entity Recognition: Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QQKnPjPDHJHr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4e511768-b39e-4e6e-eb09-d97ff0360c2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "TensorFlow 1.x selected.\n",
            "Collecting benepar\n",
            "  Downloading benepar-0.2.0.tar.gz (33 kB)\n",
            "Requirement already satisfied: nltk>=3.2 in /usr/local/lib/python3.7/dist-packages (from benepar) (3.2.5)\n",
            "Requirement already satisfied: spacy>=2.0.9 in /usr/local/lib/python3.7/dist-packages (from benepar) (2.2.4)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from benepar) (1.10.0+cu111)\n",
            "Collecting torch-struct>=0.5\n",
            "  Downloading torch_struct-0.5-py3-none-any.whl (34 kB)\n",
            "Collecting tokenizers>=0.9.4\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 22.3 MB/s \n",
            "\u001b[?25hCollecting transformers[tokenizers,torch]>=4.2.2\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 51.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from benepar) (3.17.3)\n",
            "Collecting sentencepiece>=0.1.91\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 52.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2->benepar) (1.15.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (1.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (4.62.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (57.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (2.0.6)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (3.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (1.21.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (0.9.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.9->benepar) (1.0.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.0.9->benepar) (4.11.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.0.9->benepar) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.0.9->benepar) (3.10.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (3.0.4)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 40.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (2.10)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (2019.12.20)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 50.9 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 38.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (3.6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 3.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers[tokenizers,torch]>=4.2.2->benepar) (3.0.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[tokenizers,torch]>=4.2.2->benepar) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[tokenizers,torch]>=4.2.2->benepar) (7.1.2)\n",
            "Building wheels for collected packages: benepar\n",
            "  Building wheel for benepar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for benepar: filename=benepar-0.2.0-py3-none-any.whl size=37647 sha256=1a54a01519e7e09424800cb5412353c8cd85dd806617652e50ec43770f324931\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/6f/a3/4d27ce92766bdedd2cbbbedb8857fb7a53534331191cda4994\n",
            "Successfully built benepar\n",
            "Installing collected packages: urllib3, pyyaml, tokenizers, sacremoses, huggingface-hub, transformers, torch-struct, sentencepiece, benepar\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.8\n",
            "    Uninstalling urllib3-1.26.8:\n",
            "      Successfully uninstalled urllib3-1.26.8\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "selenium 4.1.2 requires urllib3[secure,socks]~=1.26, but you have urllib3 1.25.11 which is incompatible.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.2 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed benepar-0.2.0 huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 sentencepiece-0.1.96 tokenizers-0.11.6 torch-struct-0.5 transformers-4.16.2 urllib3-1.25.11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Error loading benepar_en2: Package 'benepar_en2' not found\n",
            "[nltk_data]     in index\n",
            "[nltk_data] Downloading package benepar_en3 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/benepar/spacy_plugin.py:12: FutureWarning: BeneparComponent and NonConstituentException have been moved to the benepar module. Use `from benepar import BeneparComponent, NonConstituentException` instead of benepar.spacy_plugin. The benepar.spacy_plugin namespace is deprecated and will be removed in a future version.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data]   Unzipping models/benepar_en3.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import spacy\n",
        "nlp=spacy.load('en_core_web_sm')\n",
        "%tensorflow_version 1.x\n",
        "!pip install benepar\n",
        "import benepar\n",
        "benepar.download('benepar_en2')\n",
        "from benepar.spacy_plugin import BeneparComponent\n",
        "benepar.download('benepar_en3')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def spcy(text):\n",
        "  for t in text:\n",
        "    for token in nlp(t):\n",
        "      return [token.text, '-',token.pos_,'-',token.tag_]\n",
        "df['pos']=df['tokens'].apply(lambda x: spcy(x))\n",
        "print(df['pos'].head())\n",
        "print(len(df['pos']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66R_0qMTn8NK",
        "outputId": "15b45341-b8f9-4d09-b4dc-191a025e2574"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1    [shaun, -, PROPN, -, NNP]\n",
            "4         [i, -, PRON, -, PRP]\n",
            "5          [of, -, ADP, -, IN]\n",
            "8    [having, -, VERB, -, VBG]\n",
            "9     [martial, -, ADJ, -, JJ]\n",
            "Name: pos, dtype: object\n",
            "930\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWOtvT2rHNWy"
      },
      "source": [
        "**Write your explanations of the constituency parsing tree and dependency parsing tree here (Question 3-2):** "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "K2vPdmhBgDFy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dependency parsing is the process of analyzing the grammatical structure of a sentence based on the dependencies between the words in a sentence. \n",
        "In Dependency parsing, various tags represent the relationship between two words in a sentence. These tags are the dependency tags\n",
        "\n",
        "Constituency Parsing is the process of analyzing the sentences by breaking down it into sub-phrases also known as constituents. These sub-phrases belong to a specific category of grammar like noun phrase (NP) and verb phrase (VP) \n"
      ],
      "metadata": {
        "id": "8497jHopViiX"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "INFO5731_Assignment_Two_1 (2).ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}