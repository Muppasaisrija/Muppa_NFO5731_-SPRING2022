{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Muppasaisrija/Muppa_NFO5731_-SPRING2022/blob/main/In_class_exercise_04_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzxI8G7p42GR"
      },
      "source": [
        "# **The fourth in-class-exercise (40 points in total, 03/29/2022)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ygbo4jfQ42GU"
      },
      "source": [
        "Question description: Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnfJtqFr42GW"
      },
      "source": [
        "## (1) (10 points) Generate K topics by using LDA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here: \n",
        "\n",
        "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSnnzC-R42GX"
      },
      "outputs": [],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxW6DEbu42GZ",
        "outputId": "2362b8d6-5f95-4882-abcf-2bf07ad31c1e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>_unit_id</th>\n",
              "      <th>_golden</th>\n",
              "      <th>_unit_state</th>\n",
              "      <th>_trusted_judgments</th>\n",
              "      <th>_last_judgment_at</th>\n",
              "      <th>gender</th>\n",
              "      <th>gender:confidence</th>\n",
              "      <th>profile_yn</th>\n",
              "      <th>profile_yn:confidence</th>\n",
              "      <th>created</th>\n",
              "      <th>...</th>\n",
              "      <th>profileimage</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>sidebar_color</th>\n",
              "      <th>text</th>\n",
              "      <th>tweet_coord</th>\n",
              "      <th>tweet_count</th>\n",
              "      <th>tweet_created</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>tweet_location</th>\n",
              "      <th>user_timezone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>815719226</td>\n",
              "      <td>False</td>\n",
              "      <td>finalized</td>\n",
              "      <td>3</td>\n",
              "      <td>10/26/15 23:24</td>\n",
              "      <td>male</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>yes</td>\n",
              "      <td>1.0</td>\n",
              "      <td>12/5/13 1:48</td>\n",
              "      <td>...</td>\n",
              "      <td>https://pbs.twimg.com/profile_images/414342229...</td>\n",
              "      <td>0</td>\n",
              "      <td>FFFFFF</td>\n",
              "      <td>Robbie E Responds To Critics After Win Against...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>110964</td>\n",
              "      <td>10/26/15 12:40</td>\n",
              "      <td>6.587300e+17</td>\n",
              "      <td>main; @Kan1shk3</td>\n",
              "      <td>Chennai</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>815719227</td>\n",
              "      <td>False</td>\n",
              "      <td>finalized</td>\n",
              "      <td>3</td>\n",
              "      <td>10/26/15 23:30</td>\n",
              "      <td>male</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>yes</td>\n",
              "      <td>1.0</td>\n",
              "      <td>10/1/12 13:51</td>\n",
              "      <td>...</td>\n",
              "      <td>https://pbs.twimg.com/profile_images/539604221...</td>\n",
              "      <td>0</td>\n",
              "      <td>C0DEED</td>\n",
              "      <td>���It felt like they were my friends and I was...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7471</td>\n",
              "      <td>10/26/15 12:40</td>\n",
              "      <td>6.587300e+17</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eastern Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>815719228</td>\n",
              "      <td>False</td>\n",
              "      <td>finalized</td>\n",
              "      <td>3</td>\n",
              "      <td>10/26/15 23:33</td>\n",
              "      <td>male</td>\n",
              "      <td>0.6625</td>\n",
              "      <td>yes</td>\n",
              "      <td>1.0</td>\n",
              "      <td>11/28/14 11:30</td>\n",
              "      <td>...</td>\n",
              "      <td>https://pbs.twimg.com/profile_images/657330418...</td>\n",
              "      <td>1</td>\n",
              "      <td>C0DEED</td>\n",
              "      <td>i absolutely adore when louis starts the songs...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5617</td>\n",
              "      <td>10/26/15 12:40</td>\n",
              "      <td>6.587300e+17</td>\n",
              "      <td>clcncl</td>\n",
              "      <td>Belgrade</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>815719229</td>\n",
              "      <td>False</td>\n",
              "      <td>finalized</td>\n",
              "      <td>3</td>\n",
              "      <td>10/26/15 23:10</td>\n",
              "      <td>male</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>yes</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6/11/09 22:39</td>\n",
              "      <td>...</td>\n",
              "      <td>https://pbs.twimg.com/profile_images/259703936...</td>\n",
              "      <td>0</td>\n",
              "      <td>C0DEED</td>\n",
              "      <td>Hi @JordanSpieth - Looking at the url - do you...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1693</td>\n",
              "      <td>10/26/15 12:40</td>\n",
              "      <td>6.587300e+17</td>\n",
              "      <td>Palo Alto, CA</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>815719230</td>\n",
              "      <td>False</td>\n",
              "      <td>finalized</td>\n",
              "      <td>3</td>\n",
              "      <td>10/27/15 1:15</td>\n",
              "      <td>female</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>yes</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4/16/14 13:23</td>\n",
              "      <td>...</td>\n",
              "      <td>https://pbs.twimg.com/profile_images/564094871...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Watching Neighbours on Sky+ catching up with t...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>31462</td>\n",
              "      <td>10/26/15 12:40</td>\n",
              "      <td>6.587300e+17</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20045</th>\n",
              "      <td>815757572</td>\n",
              "      <td>True</td>\n",
              "      <td>golden</td>\n",
              "      <td>259</td>\n",
              "      <td>NaN</td>\n",
              "      <td>female</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>yes</td>\n",
              "      <td>1.0</td>\n",
              "      <td>8/5/15 21:16</td>\n",
              "      <td>...</td>\n",
              "      <td>https://pbs.twimg.com/profile_images/656793310...</td>\n",
              "      <td>0</td>\n",
              "      <td>C0DEED</td>\n",
              "      <td>@lookupondeath ...Fine, and I'll drink tea too...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>783</td>\n",
              "      <td>10/26/15 13:20</td>\n",
              "      <td>6.587400e+17</td>\n",
              "      <td>Verona ���</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20046</th>\n",
              "      <td>815757681</td>\n",
              "      <td>True</td>\n",
              "      <td>golden</td>\n",
              "      <td>248</td>\n",
              "      <td>NaN</td>\n",
              "      <td>male</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>yes</td>\n",
              "      <td>1.0</td>\n",
              "      <td>8/15/12 21:17</td>\n",
              "      <td>...</td>\n",
              "      <td>https://pbs.twimg.com/profile_images/639815429...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Greg Hardy you a good player and all but don't...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13523</td>\n",
              "      <td>10/26/15 12:40</td>\n",
              "      <td>6.587300e+17</td>\n",
              "      <td>Kansas City, MO</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20047</th>\n",
              "      <td>815757830</td>\n",
              "      <td>True</td>\n",
              "      <td>golden</td>\n",
              "      <td>264</td>\n",
              "      <td>NaN</td>\n",
              "      <td>male</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>yes</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9/3/12 1:17</td>\n",
              "      <td>...</td>\n",
              "      <td>https://pbs.twimg.com/profile_images/655473271...</td>\n",
              "      <td>0</td>\n",
              "      <td>C0DEED</td>\n",
              "      <td>You can miss people and still never want to se...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>26419</td>\n",
              "      <td>10/26/15 13:20</td>\n",
              "      <td>6.587400e+17</td>\n",
              "      <td>Lagos Nigeria</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20048</th>\n",
              "      <td>815757921</td>\n",
              "      <td>True</td>\n",
              "      <td>golden</td>\n",
              "      <td>250</td>\n",
              "      <td>NaN</td>\n",
              "      <td>female</td>\n",
              "      <td>0.8489</td>\n",
              "      <td>yes</td>\n",
              "      <td>1.0</td>\n",
              "      <td>11/6/12 23:46</td>\n",
              "      <td>...</td>\n",
              "      <td>https://pbs.twimg.com/profile_images/657716093...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>@bitemyapp i had noticed your tendency to pee ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>56073</td>\n",
              "      <td>10/26/15 12:40</td>\n",
              "      <td>6.587300e+17</td>\n",
              "      <td>Texas Hill Country</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20049</th>\n",
              "      <td>815757985</td>\n",
              "      <td>True</td>\n",
              "      <td>golden</td>\n",
              "      <td>249</td>\n",
              "      <td>NaN</td>\n",
              "      <td>female</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>yes</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4/14/14 17:22</td>\n",
              "      <td>...</td>\n",
              "      <td>https://pbs.twimg.com/profile_images/655134724...</td>\n",
              "      <td>0</td>\n",
              "      <td>C0DEED</td>\n",
              "      <td>I think for my APUSH creative project I'm goin...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2922</td>\n",
              "      <td>10/26/15 13:19</td>\n",
              "      <td>6.587400e+17</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20050 rows × 26 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
              "0      815719226    False   finalized                   3    10/26/15 23:24   \n",
              "1      815719227    False   finalized                   3    10/26/15 23:30   \n",
              "2      815719228    False   finalized                   3    10/26/15 23:33   \n",
              "3      815719229    False   finalized                   3    10/26/15 23:10   \n",
              "4      815719230    False   finalized                   3     10/27/15 1:15   \n",
              "...          ...      ...         ...                 ...               ...   \n",
              "20045  815757572     True      golden                 259               NaN   \n",
              "20046  815757681     True      golden                 248               NaN   \n",
              "20047  815757830     True      golden                 264               NaN   \n",
              "20048  815757921     True      golden                 250               NaN   \n",
              "20049  815757985     True      golden                 249               NaN   \n",
              "\n",
              "       gender  gender:confidence profile_yn  profile_yn:confidence  \\\n",
              "0        male             1.0000        yes                    1.0   \n",
              "1        male             1.0000        yes                    1.0   \n",
              "2        male             0.6625        yes                    1.0   \n",
              "3        male             1.0000        yes                    1.0   \n",
              "4      female             1.0000        yes                    1.0   \n",
              "...       ...                ...        ...                    ...   \n",
              "20045  female             1.0000        yes                    1.0   \n",
              "20046    male             1.0000        yes                    1.0   \n",
              "20047    male             1.0000        yes                    1.0   \n",
              "20048  female             0.8489        yes                    1.0   \n",
              "20049  female             1.0000        yes                    1.0   \n",
              "\n",
              "              created  ...                                       profileimage  \\\n",
              "0        12/5/13 1:48  ...  https://pbs.twimg.com/profile_images/414342229...   \n",
              "1       10/1/12 13:51  ...  https://pbs.twimg.com/profile_images/539604221...   \n",
              "2      11/28/14 11:30  ...  https://pbs.twimg.com/profile_images/657330418...   \n",
              "3       6/11/09 22:39  ...  https://pbs.twimg.com/profile_images/259703936...   \n",
              "4       4/16/14 13:23  ...  https://pbs.twimg.com/profile_images/564094871...   \n",
              "...               ...  ...                                                ...   \n",
              "20045    8/5/15 21:16  ...  https://pbs.twimg.com/profile_images/656793310...   \n",
              "20046   8/15/12 21:17  ...  https://pbs.twimg.com/profile_images/639815429...   \n",
              "20047     9/3/12 1:17  ...  https://pbs.twimg.com/profile_images/655473271...   \n",
              "20048   11/6/12 23:46  ...  https://pbs.twimg.com/profile_images/657716093...   \n",
              "20049   4/14/14 17:22  ...  https://pbs.twimg.com/profile_images/655134724...   \n",
              "\n",
              "       retweet_count sidebar_color  \\\n",
              "0                  0        FFFFFF   \n",
              "1                  0        C0DEED   \n",
              "2                  1        C0DEED   \n",
              "3                  0        C0DEED   \n",
              "4                  0             0   \n",
              "...              ...           ...   \n",
              "20045              0        C0DEED   \n",
              "20046              0             0   \n",
              "20047              0        C0DEED   \n",
              "20048              0             0   \n",
              "20049              0        C0DEED   \n",
              "\n",
              "                                                    text tweet_coord  \\\n",
              "0      Robbie E Responds To Critics After Win Against...         NaN   \n",
              "1      ���It felt like they were my friends and I was...         NaN   \n",
              "2      i absolutely adore when louis starts the songs...         NaN   \n",
              "3      Hi @JordanSpieth - Looking at the url - do you...         NaN   \n",
              "4      Watching Neighbours on Sky+ catching up with t...         NaN   \n",
              "...                                                  ...         ...   \n",
              "20045  @lookupondeath ...Fine, and I'll drink tea too...         NaN   \n",
              "20046  Greg Hardy you a good player and all but don't...         NaN   \n",
              "20047  You can miss people and still never want to se...         NaN   \n",
              "20048  @bitemyapp i had noticed your tendency to pee ...         NaN   \n",
              "20049  I think for my APUSH creative project I'm goin...         NaN   \n",
              "\n",
              "      tweet_count   tweet_created      tweet_id      tweet_location  \\\n",
              "0          110964  10/26/15 12:40  6.587300e+17     main; @Kan1shk3   \n",
              "1            7471  10/26/15 12:40  6.587300e+17                 NaN   \n",
              "2            5617  10/26/15 12:40  6.587300e+17              clcncl   \n",
              "3            1693  10/26/15 12:40  6.587300e+17       Palo Alto, CA   \n",
              "4           31462  10/26/15 12:40  6.587300e+17                 NaN   \n",
              "...           ...             ...           ...                 ...   \n",
              "20045         783  10/26/15 13:20  6.587400e+17          Verona ���   \n",
              "20046       13523  10/26/15 12:40  6.587300e+17     Kansas City, MO   \n",
              "20047       26419  10/26/15 13:20  6.587400e+17      Lagos Nigeria    \n",
              "20048       56073  10/26/15 12:40  6.587300e+17  Texas Hill Country   \n",
              "20049        2922  10/26/15 13:19  6.587400e+17                 NaN   \n",
              "\n",
              "                    user_timezone  \n",
              "0                         Chennai  \n",
              "1      Eastern Time (US & Canada)  \n",
              "2                        Belgrade  \n",
              "3      Pacific Time (US & Canada)  \n",
              "4                             NaN  \n",
              "...                           ...  \n",
              "20045                         NaN  \n",
              "20046                         NaN  \n",
              "20047                         NaN  \n",
              "20048                         NaN  \n",
              "20049                         NaN  \n",
              "\n",
              "[20050 rows x 26 columns]"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "#import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "import logging\n",
        "import argparse\n",
        "\n",
        "#load csv file and create dataframe\n",
        "df = pd.read_csv(r'C:\\Users\\admin\\Downloads\\archive (5)\\tweet_data.csv')\n",
        "df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgZ5IbS742Gc"
      },
      "outputs": [],
      "source": [
        "##Generate K topics by using LDA, the number of topics K should be decided by the coherence score, then summarize what are the topics\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "\n",
        "K = 10\n",
        "def lda_clustering(df, K):\n",
        "    #create a count matrix from the corpus\n",
        "    count_vectorizer = CountVectorizer()\n",
        "    count_matrix = count_vectorizer.fit_transform(df['text'])\n",
        "    #use tfidf to normalize the word counts\n",
        "    tfidf_transformer = TfidfTransformer()\n",
        "    tfidf_matrix = tfidf_transformer.fit_transform(count_matrix)\n",
        "    #compute the similarity between the documents\n",
        "    #cosine_similarity(tfidf_matrix)\n",
        "    #create a lda model with K topics\n",
        "    lda = LatentDirichletAllocation(n_components=K, learning_method='online', learning_offset=50.,random_state=0).fit(tfidf_matrix)\n",
        "    #get the topics for each document\n",
        "    lda_output = lda.transform(tfidf_matrix)\n",
        "    #get the top 10 words for each topic\n",
        "    topic_word = lda.components_\n",
        "    vocab = count_vectorizer.get_feature_names()\n",
        "    for topic_idx, topic in enumerate(topic_word):\n",
        "        print(\"Topic %d:\" % (topic_idx))\n",
        "        print(\" \".join([vocab[i] for i in topic.argsort()[:-10 - 1:-1]]))\n",
        "    #get the top 10 documents for each topic\n",
        "    doc_topic = lda.transform(tfidf_matrix)\n",
        "    for i, topic_dist in enumerate(doc_topic):\n",
        "        topic_words = np.array(vocab)[np.argsort(topic_dist)][:-10:-1]\n",
        "        print('Document {}: {}'.format(i, ' '.join(topic_words)))\n",
        "    #get the coherence score\n",
        "    coherence_model_lda = CoherenceModel(model=lda, texts=df['text'], dictionary=count_vectorizer.vocabulary_, coherence='c_v')\n",
        "    coherence_lda = coherence_model_lda.get_coherence()\n",
        "    print('\\nCoherence Score: ', coherence_lda)\n",
        "\n",
        "    return lda_output, topic_word, doc_topic\n",
        "\n",
        "lda_clustering(df, K)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wn70-CzW42Gf"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Generate K topics by using LDA, the number of topics K should be decided by the coherence score, then summarize what are the topics\n",
        "#pip install gensim\n",
        "\n",
        "from gensim import corpora, models\n",
        "#the number of topics K should be decided by the coherence score of the topics\n",
        "#the coherence score is the sum of the log of the probability of the words in the topic\n",
        "K = 5\n",
        "#Tokenize the sentence into words\n",
        "\n",
        "\n",
        "#tokenazize the dataset\n",
        "df['tokenized'] = df['text'].apply(lambda x: x.split())\n",
        "\n",
        "#create a dictionary of the tokens\n",
        "dictionary = df['tokenized'].apply(lambda x: list(set(x)))\n",
        "\n",
        "#dictionary = corpora.Dictionary(df['Text'].split())\n",
        "print(dictionary)\n",
        "corpus = dictionary.apply(lambda x: list(dictionary.index)).apply(lambda x: list(zip(x, [1]*len(x))))\n",
        "\n",
        "\n",
        "lda = models.LdaModel(corpus, num_topics=K, id2word=dictionary, passes=10)\n",
        "\n",
        "def print_topics(model, top_n=10):\n",
        "    for idx, topic in enumerate(model.top_topics(num_words=top_n)):\n",
        "        print('Topic {}: {}'.format(idx, ' '.join([word for word, prop in topic[1]])))\n",
        "print_topics(lda)\n",
        "#then summarize what are the topics\n",
        "lda.print_topics(num_topics=K, num_words=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIX9CsAB42Gi"
      },
      "source": [
        "## (2) (10 points) Generate K topics by using LSA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_n7eegN742Gj"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "\n",
        "#Generate K topics by using LSA, the number of topics K should be decided by the coherence score, then summarize what are the topics.\n",
        "#import libraries\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import Normalizer\n",
        "#import LSA\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from gensim import corpora, models\n",
        "#Generate K topics by using LSA, the number of topics K should be decided by the coherence score\n",
        "K = 5\n",
        "#the coherence score is the sum of the log of the probability of the words in the topic\n",
        "lsa = TruncatedSVD(n_components=K, n_iter=10, random_state=42)\n",
        "#create a pipeline to normalize the tf-idf vectors\n",
        "pipeline = make_pipeline(TfidfVectorizer(), Normalizer(copy=False), lsa)\n",
        "#fit the pipeline to the data\n",
        "pipeline.fit(df['Text'])\n",
        "#summarize what are the topics\n",
        "print_topics(lsa)\n",
        "#then summarize what are the topics\n",
        "lsa.print_topics(num_topics=K, num_words=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJ7T-6dz42Gk"
      },
      "source": [
        "## (3) (10 points) Generate K topics by using  lda2vec, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "https://nbviewer.org/github/cemoody/lda2vec/blob/master/examples/twenty_newsgroups/lda2vec/lda2vec.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMRm-dhw42Gl"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "#Generate K topics by using  lda2vec, the number of topics K should be decided by the coherence score, then summarize what are the topics\n",
        "# import ldavec\n",
        "from gensim.models import LdaMulticore\n",
        "from gensim.models.wrappers import LdaVec\n",
        "#the number of topics K should be decided by the coherence score of the topics\n",
        "#the coherence score is the sum of the log of the probability of the words in the topic\n",
        "K = 5\n",
        "#the number of iterations\n",
        "iterations = 10\n",
        "#the number of passes\n",
        "passes = 10\n",
        "lda2vec = LdaVec(corpus=corpus, num_topics=K, id2word=dictionary, passes=passes, iterations=iterations)\n",
        "# summarize what are the topics\n",
        "print_topics(lda2vec)\n",
        "#then summarize what are the topics\n",
        "lda2vec.print_topics(num_topics=K, num_words=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY-aI0tL42Gn"
      },
      "source": [
        "## (4) (10 points) Generate K topics by using BERTopic, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here: \n",
        "\n",
        "https://colab.research.google.com/drive/1FieRA9fLdkQEGDIMYl0I3MCjSUKVF8C-?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gqdd1fCz42Gn"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "#Generate K topics by using BERTopic, the number of topics K should be decided by the coherence score, then summarize what are the topics\n",
        "#import libraries\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "# import BERTopic\n",
        "from gensim.models import LdaMulticore\n",
        "from gensim.models.wrappers import LdaVec\n",
        "#the number of topics K should be decided by the coherence score of the topics\n",
        "\n",
        "BERTopic = LdaVec(corpus=corpus, num_topics=K, id2word=dictionary, passes=10, iterations=10)\n",
        "# summarize what are the topics\n",
        "print_topics(BERTopic)\n",
        "#then summarize what are the topics\n",
        "BERTopic.print_topics(num_topics=K, num_words=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReCfCygs42Gp"
      },
      "source": [
        "## (5) (10 extra points) Compare the results generated by the four topic modeling algorithms, which one is better? You should explain the reasons in details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pw33hnUY42Gq"
      },
      "outputs": [],
      "source": [
        "# Write your answer here (no code needed for this question)\n",
        "# Compare the results generated by the four topic modeling algorithms\n",
        "\"\"\"\n",
        "The results are similar.\n",
        "\"\"\"\n",
        "#which is better between lda,lsa?\n",
        "\"\"\"\n",
        "LSA.\n",
        "\n",
        "in Natural Language Processing (NLP): Latent Semantic Analysis (LSA), \n",
        "Latent Dirichlet Allocation (LDA) and lexical chains. These techniques were evaluated \n",
        "and compared on two different corpora in order to highlight the similarities and differences \n",
        "between them from a semantic analysis viewpoint. The first corpus consisted of four Wikipedia \n",
        "articles on different topics, while the second one consisted of 35 online chat conversations \n",
        "between 4-12 participants debating four imposed topics (forum, chat, blog and wikis). The study \n",
        "focuses on finding similarities and differences between the outcomes of the three methods from a \n",
        "semantic analysis point of view, by computing quantitative factors such as correlations, degree of \n",
        "coverage of the resulting topics, etc. Using corpora from different types of discourse and quantitative \n",
        "factors that are task-independent allows us to prove that although LSA and LDA provide similar results,\n",
        "the results of lexical chaining are not very correlated with neither the ones of LSA or LDA, therefore \n",
        "lexical chains might be used complementary to LSA or LDA when performing semantic analysis for various NLP applications.\n",
        "These factors out to be the reason why the LSA are the better.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAplh0Mm42Gr"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "In_class_exercise_04 .ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}